{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10767740,"sourceType":"datasetVersion","datasetId":4987536},{"sourceId":10768018,"sourceType":"datasetVersion","datasetId":5902635}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ayushkhaire/real-time-data-update?scriptVersionId=223166016\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import requests as rq\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime\nimport time \nfrom tqdm import tqdm\nimport os\nfrom datetime import datetime,timedelta\nfrom pymongo.mongo_client import MongoClient\nfrom pymongo.server_api import ServerApi\nimport logging\nfrom kaggle_secrets import UserSecretsClient\nimport warnings\nimport json\nimport shutil\nimport subprocess\nimport gc\n\nwarnings.filterwarnings('ignore')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:45:42.63893Z","iopub.execute_input":"2025-02-18T07:45:42.639431Z","iopub.status.idle":"2025-02-18T07:45:44.045611Z","shell.execute_reply.started":"2025-02-18T07:45:42.639389Z","shell.execute_reply":"2025-02-18T07:45:44.044421Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(message)s\")\nlogger = logging.getLogger(__name__)\n\n\nlogger.info(\"This is an INFO message\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:45:44.047991Z","iopub.execute_input":"2025-02-18T07:45:44.049162Z","iopub.status.idle":"2025-02-18T07:45:44.055237Z","shell.execute_reply.started":"2025-02-18T07:45:44.049114Z","shell.execute_reply":"2025-02-18T07:45:44.053823Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Secrets","metadata":{}},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nkaggle_apikey = user_secrets.get_secret(\"kaggle_apikey\")\nkaggle_username = user_secrets.get_secret(\"kaggle_username\")\nmngodb_database_name = user_secrets.get_secret(\"mngodb_database_name\")\nmongodb_app_name = user_secrets.get_secret(\"mongodb_appname\")\nmongodb_password = user_secrets.get_secret(\"mongodb_password\")\nmongodb_username = user_secrets.get_secret(\"mongodb_username\")\nmongodb_cluster_name = user_secrets.get_secret(\"mongodb_cluster_name\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:45:44.057098Z","iopub.execute_input":"2025-02-18T07:45:44.057583Z","iopub.status.idle":"2025-02-18T07:45:45.302247Z","shell.execute_reply.started":"2025-02-18T07:45:44.057532Z","shell.execute_reply":"2025-02-18T07:45:45.301003Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# MongoDB","metadata":{}},{"cell_type":"code","source":"class AtlasClient:\n    def __init__(self, atlas_uri, dbname):\n        self.mongodb_client = MongoClient(atlas_uri)\n        self.database = self.mongodb_client[dbname]\n\n    def ping(self):\n        try:\n            self.mongodb_client.admin.command('ping')\n            logging.info(\"Pinged your MongoDB deployment. Connection successful.\")\n        except Exception as e:\n            logging.error(f\"Failed to connect to MongoDB: {e}\")\n\n    def get_collection(self, collection_name):\n        collection = self.database[collection_name]\n        return collection\n\n    def findOneByKey(self,collection_name,key):\n        collection = self.get_collection(collection_name)\n        result = collection.find_one({ key: { \"$exists\": True } })\n        return result\n\n\n    def find(self, collection_name, filter={}, limit=0):\n        collection = self.database[collection_name]\n        items = list(collection.find(filter=filter, limit=limit))\n        return items\n    \n    def insert(self, collection_name, documents):\n        \"\"\"\n        Inserts one or more documents into a MongoDB collection.\n        \n        Parameters:\n        - collection_name: str, the name of the collection\n        - documents: dict or list of dicts, the document(s) to insert\n        \n        If `documents` is a list, it will insert multiple documents using `insert_many`.\n        Otherwise, it will insert a single document using `insert_one`.\n        \"\"\"\n        collection = self.get_collection(collection_name)\n        \n        if isinstance(documents, list):\n            result = collection.insert_many(documents)\n            return result.inserted_ids\n        else:\n            result = collection.insert_one(documents)\n            return result.inserted_id\n        \n    def delete(self, collection_name, filter={}, _del_all_=False):\n        \"\"\"\n        Deletes documents from a MongoDB collection based on the filter.\n        \n        Parameters:\n        - collection_name: str, the name of the collection.\n        - filter: dict, the filter to find documents to delete (default is {}).\n        - _del_all_: bool, if True, deletes all documents matching the filter using `delete_many()`.\n                      If False, deletes only one document using `delete_one()`.\n        \n        Returns:\n        - Number of documents deleted.\n        \"\"\"\n        collection = self.get_collection(collection_name)\n        \n        if _del_all_:\n            result = collection.delete_many(filter)\n            return result.deleted_count\n        else:\n            result = collection.delete_one(filter)\n            if result.deleted_count == 1:\n                pass\n            else:\n                pass\n            return result.deleted_count","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:45:45.303791Z","iopub.execute_input":"2025-02-18T07:45:45.304231Z","iopub.status.idle":"2025-02-18T07:45:45.31746Z","shell.execute_reply.started":"2025-02-18T07:45:45.304195Z","shell.execute_reply":"2025-02-18T07:45:45.315958Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Stocks manager","metadata":{}},{"cell_type":"code","source":"AC = AtlasClient(\n    atlas_uri=f\"mongodb+srv://{mongodb_username}:{mongodb_password}@{mongodb_cluster_name}.fznbh.mongodb.net/?retryWrites=true&w=majority&appName={mongodb_app_name}\",\n    dbname = mngodb_database_name\n)\n\n\nclass stocksManager:\n    def __init__(self) -> None:\n        self.available_stocks = []\n        headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n        }\n        self.headers = headers\n        self.firstrun = 0\n\n    def collect_stock_symbols(self):\n        targets = [\n            '52-week-gainers', \n            '52-week-losers'\n        ]   \n    \n        limitlist = []\n\n        for page in tqdm(targets):\n            url = f'https://finance.yahoo.com/markets/stocks/{page}/?start=0&count=100'\n            # print(url)\n            try:\n                r = rq.get(url,headers = self.headers)\n            except Exception as e:\n                logger.warning(\"cannot hit url : \",url ,e,r.status_code)\n            soup = BeautifulSoup(r.text,'html.parser')\n            limits = soup.find(\n                'div',{'class':'total'}\n            ).text\n            limits = limits.split(' ')[2]\n            limitlist.append(limits)\n\n        max_hits = []\n        for limit in limitlist:\n            max_hit = int(int(limit) / 100)\n            max_hits.append(max_hit)\n\n        findict = {\n            'targets':targets,\n            'max_hits':max_hits\n        }\n        \n        urls_for_stocks = []\n\n        i = 0\n        for i in range(\n            len(\n                findict['targets']\n                )\n            ):\n            target = findict['targets'][i]\n            maxhit = findict['max_hits'][i]\n            for m in range(maxhit+1):\n                url = f'https://finance.yahoo.com/markets/stocks/{target}/?start={m*100}&count=100/'\n                urls_for_stocks.append(url)\n\n        data = []\n\n        logger.info('collecting data for symbols _______________________________--')\n        for u in urls_for_stocks:\n            catg = u.split('/')[-3]\n            symbol_list = []\n            try:\n                r = rq.get(u,headers = self.headers)\n            except Exception as e:\n                logger.warning(\"cannot hit url : \",u ,r.status_code)\n            soup = BeautifulSoup(r.text,'html.parser')\n            symbs= soup.find_all('span',{'class':'symbol'})\n            for s in symbs:\n                symbol_list.append(s.text)\n            data.append(\n                {catg:symbol_list}\n            )\n        logger.info(\"finished collecting data for symbols ______________________________-\")\n        data = {'names':data}\n        return data\n    \n    def return_list_for_symbols(self):\n        symbols = self.collect_stock_symbols()\n        finals_symbols = []\n        for n in symbols['names']:\n            for key in n.keys():\n                finals_symbols=finals_symbols+n[key]\n        finals_symbols = list(set(finals_symbols))\n        return finals_symbols\n\n    def return_human_timestamp(self, timestamps):\n            if isinstance(timestamps, list):\n                new_dates = []\n                for unix_time in timestamps:\n                    try:\n                        if isinstance(unix_time, str):\n                            datetime.strptime(unix_time, '%Y-%m-%d %H:%M:%S') \n                            new_dates.append(unix_time)\n                        else:\n                            unix_time = float(unix_time)\n                            date = datetime.fromtimestamp(unix_time).strftime('%Y-%m-%d %H:%M:%S')\n                            new_dates.append(date)\n                    except (ValueError, TypeError):\n                        new_dates.append(None)  \n                return new_dates\n            elif isinstance(timestamps, str):\n                try:\n                    unix_time = float(timestamps)\n                    date = datetime.fromtimestamp(unix_time).strftime('%Y-%m-%d %H:%M:%S')\n                    return date\n                except (ValueError, TypeError):\n                    return None\n\n    def return_unix_timestamps(self, date_strings):\n        if isinstance(date_strings, list):\n            unix_timestamps = []\n            for date_str in date_strings:\n                try:\n                    dt = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')\n                    unix_timestamp = int(dt.timestamp())  \n                    unix_timestamps.append(unix_timestamp)\n                except (ValueError, TypeError):\n                    unix_timestamps.append(None)\n            return unix_timestamps\n        elif isinstance(date_strings, str):\n            try:\n                dt = datetime.strptime(date_strings, '%Y-%m-%d %H:%M:%S')\n                unix_timestamp = int(dt.timestamp())  \n                return unix_timestamp\n            except (ValueError, TypeError):\n                return None\n\n    def update_prices_for_daily(self, symbol_list):\n        current_timestamp = int(time.time())\n        current_time = datetime.fromtimestamp(current_timestamp)\n        human_readable_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n        \n        # Start and end periods for data retrieval\n        start_date_str = \"2015-01-01\"\n        start_date_obj = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n        period1 = int(time.mktime(start_date_obj.timetuple()))\n        period2 = current_timestamp\n        \n        logger.warning(f\"Daily data for today's date {human_readable_time}\")\n        logger.info(f\"Checking updates for period1={period1} & period2={period2} for stocks daily\")\n\n        # Define base path for daily updates\n        files_path = f'/kaggle/working/daily_update/'\n        os.makedirs('/kaggle/working/daily_update/',exist_ok = True)\n        os.makedirs('/kaggle/working/daily_update_to_kaggle/',exist_ok = True)\n        AC.delete(\n            \"daily_data\",\n            _del_all_ = True\n        )\n        for stock in tqdm(symbol_list):\n            stock_symbol = stock.replace(' ', '')\n            json_path = f'{files_path}/{stock_symbol}.json'\n            os.makedirs(os.path.dirname(json_path), exist_ok=True)\n            \n            url = (f'https://query1.finance.yahoo.com/v8/finance/chart/{stock_symbol}?events=capitalGain%7Cdiv%7Csplit'\n                f'&formatted=true&includeAdjustedClose=true&interval=1d&period1={period1}&period2={period2}'\n                f'&symbol={stock_symbol}&userYfid=true&lang=en-US&region=US')\n            try:\n                response = rq.get(url, headers=self.headers)\n                if response.status_code == 200:\n                    with open(json_path, 'wb') as file:\n                        file.write(response.content)\n                    json_data = pd.read_json(json_path)\n                    timestamp = json_data['chart']['result'][0].get('timestamp')\n                    if timestamp:\n                        new_timestamps = self.return_human_timestamp(timestamp)\n                        new_data = json_data['chart']['result'][0]['indicators']['quote'][0]\n                        new_data['timestamp'] = new_timestamps\n                        data_to_insert = {f'{stock_symbol}':new_data}\n                        if data_to_insert:\n#                             in database\n                            AC.insert(\n                                collection_name=\"daily_data\",\n                                documents=data_to_insert\n                            )\n#                            local\n                            new_data = pd.DataFrame(new_data)\n                            new_data.to_csv(f'/kaggle/working/daily_update_to_kaggle/{stock}.csv')\n                            \n                        else:\n                            logger.error(f'daily data insertion for {stock} failed .',e)\n                else:\n                    logger.warning(f\"Request failed: {url}, Status code: {response.status_code}\")\n                    continue\n            except:\n                continue\n        logger.info(\"Daily data update finished.\")\n  \n    \n    def update_prices_for_per_minute(self, symbol_list,last_date):\n        os.makedirs(f'/kaggle/working/per_minute/', exist_ok=True)\n        os.makedirs(f'/kaggle/working/per_minute_to_kaggle/', exist_ok=True) \n        date_time_obj = datetime.strptime(last_date, '%Y-%m-%d %H:%M:%S')\n        period1 = int(date_time_obj.timestamp())\n        seven_days_back = date_time_obj - timedelta(days=7)\n        period2 = int(seven_days_back.timestamp())\n  \n        logger.info(f\"Checking updates for period1={period1} & period2={period2} for stocks per minute.\")\n            \n        AC.delete(\n                collection_name=\"per_minute_data\",\n                _del_all_ = True\n        )\n        if symbol_list:\n            for stock in tqdm(symbol_list):\n                try:\n                    stock_symbol = stock.replace(' ', '')\n                    link = f'https://query2.finance.yahoo.com/v8/finance/chart/{stock_symbol}?period1={period2}&period2={period1}&interval=1m&includePrePost=true&events=div%7Csplit%7Cearn&&lang=en-US&region=US'\n                    response = rq.get(link, headers=self.headers)\n                    tmppath = f'/kaggle/working/per_minute/{stock_symbol}.json'\n                    if response.status_code == 200:\n                        with open(tmppath, 'wb') as jsn:\n                            jsn.write(response.content)\n                        json_data  = pd.read_json(tmppath)\n                        timestamp = json_data['chart'][0][0]['timestamp']\n                        json_data = json_data['chart'][0][0][\"indicators\"][\"quote\"][0]\n                        try:\n                            new_timestamps = self.return_human_timestamp(timestamp)\n                            json_data['timestamp'] = new_timestamps\n                            data_to_insert = {f'{stock_symbol}':json_data}\n    #                         to database\n                            if data_to_insert:\n                            #    AC.insert(\n                            #         collection_name=\"per_minute_data\",\n                            #         documents=data_to_insert\n                            #    )\n    #                           to csv\n                                json_data = pd.DataFrame(json_data)\n                                json_data.to_csv(f'/kaggle/working/per_minute_to_kaggle/{stock}.csv')\n                            else:\n                                logger.warning(f'per minute data insertion data insertion for {stock} failed .'),e\n\n                        except Exception as e:\n                            logger.warning(f\"Request failed: {link}, Status code: {response.status_code}\")\n                            print('failed',e)\n                            continue\n                except:\n                    continue\n                \n        else:\n            logger.warning(\"It is not Sunday today. Skipping the update step.\")\n        logger.info(\"Per minute update finished.\")\n\n    def update_stocks_list_for_today(self):\n        AC.delete('master',_del_all_ = True)\n        stocks = AC.find(\"daily_data\")\n        stockslist = []\n        for st in tqdm(stocks):\n            stockslist.append(list(st.keys())[1])\n        self.available_stocks = stockslist\n        AC.insert(\"master\",{'stocks':stockslist})\n        logger.warning(\"stocks list updated !\")\n        \n    # specific to kaggle\n    def Kaggle_process_daily_data(self,symbol_list):\n        self.update_prices_for_daily(symbol_list)\n        megadatadailyframe = pd.DataFrame()\n        daily_files_csv = os.listdir('/kaggle/working/daily_update_to_kaggle')\n\n        for csv in tqdm(daily_files_csv):\n            df = pd.read_csv(f'/kaggle/working/daily_update_to_kaggle/{csv}')\n            df['stockname'] = csv.split('.')[0]\n            megadatadailyframe = pd.concat([megadatadailyframe,df],axis = 0)\n            \n        os.makedirs(f'/kaggle/working/daily_update_to_kaggle_final',exist_ok = True)\n        megadatadailyframe.to_csv('/kaggle/working/daily_update_to_kaggle_final/stocks.csv')\n    \n    def Kaggle_process_per_minute_data(self,symbol_list,last_date,weekday):\n        self.update_prices_for_per_minute(symbol_list,last_date)\n        megadataperminuteframe = pd.DataFrame()\n        perminute_files_csv = os.listdir('/kaggle/working/per_minute_to_kaggle/')\n\n        for csv in tqdm(perminute_files_csv):\n            df = pd.read_csv(f'/kaggle/working/per_minute_to_kaggle/{csv}')\n            df['stockname'] = csv.split('.')[0]\n            megadataperminuteframe = pd.concat([megadataperminuteframe,df],axis = 0)\n        os.makedirs(f'/kaggle/working/per_minute_to_kaggle_final',exist_ok = True)\n        past_df = pd.read_csv(\"/kaggle/input/real-time-stocks-data/stocks.csv\")\n        megadataperminuteframe = pd.concat([megadataperminuteframe,past_df],axis = 0)\n        megadataperminuteframe = megadataperminuteframe.drop_duplicates()\n        megadataperminuteframe = megadataperminuteframe[['low','high','volume','open','close','stockname','timestamp']]\n        megadataperminuteframe.to_csv('/kaggle/working/per_minute_to_kaggle_final/stocks.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:45:45.320999Z","iopub.execute_input":"2025-02-18T07:45:45.321415Z","iopub.status.idle":"2025-02-18T07:45:45.433649Z","shell.execute_reply.started":"2025-02-18T07:45:45.321377Z","shell.execute_reply":"2025-02-18T07:45:45.43243Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Recover data if any lost","metadata":{}},{"cell_type":"code","source":"class recoverData:\n    def __init__(self):\n        self.symbol_list = None\n        self.avdata = None\n        self.headers = {'User-Agent': 'Mozilla/5.0'}\n        self.batches_to_check = []\n        self.recoverlist = []\n\n    def setup(self):\n        os.makedirs(\"/kaggle/working/per_minute_recover/json/\", exist_ok=True)\n        os.makedirs(\"/kaggle/working/per_minute_recover/csv/\", exist_ok=True)\n        os.makedirs(\"/kaggle/working/per_minute_recover/final/\", exist_ok=True)\n        print(\"set up directories\")\n        self.avdata = pd.read_csv('/kaggle/input/real-time-stocks-data/stocks.csv')\n        self.symbol_list = self.avdata['stockname'].unique()\n        print(\"stock list and dataset loaded .\")\n\n    def return_unix_timestamps(self, date_strings):\n        try:\n            dt = datetime.strptime(date_strings, '%Y-%m-%d')\n            return int(dt.timestamp())\n        except ValueError:\n            return None\n\n    def return_human_timestamp(self, timestamps):\n        try:\n            return [datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S') for ts in timestamps]\n        except Exception:\n            return None\n\n    def collect_per_minute_data(self, stock_symbol, start_date, end_date):\n        period1 = start_date\n        period2 = end_date\n        if period1 is None or period2 is None:\n            print(f\"Invalid dates provided: {start_date}, {end_date}\")\n            return\n\n        stock_symbol = stock_symbol.replace(' ', '')\n        link = f\"https://query2.finance.yahoo.com/v8/finance/chart/{stock_symbol}?period1={period1}&period2={period2}&interval=1m&includePrePost=true&events=div%7Csplit%7Cearnings&lang=en-US&region=US\"\n        response = rq.get(link, headers=self.headers)\n\n        if response.status_code == 200:\n            tmppath = f'/kaggle/working/per_minute_recover/json/{stock_symbol}.json'\n            with open(tmppath, 'wb') as jsn:\n                jsn.write(response.content)\n            json_data = response.json()\n\n            try:\n                timestamps = json_data['chart']['result'][0]['timestamp']\n                indicators = json_data['chart']['result'][0]['indicators']['quote'][0]\n                indicators['timestamp'] = self.return_human_timestamp(timestamps)\n\n                df = pd.DataFrame(indicators)\n                os.makedirs(f'/kaggle/working/per_minute_recover/csv/{stock_symbol}/',exist_ok = True)\n                df.to_csv(\n                    f\"/kaggle/working/per_minute_recover/csv/{stock_symbol}/{start_date}_{end_date}.csv\",\n                    index=False,\n                )\n            except KeyError as e:\n                print(f\"Error processing JSON data: {e}\")\n        else:\n            print(f\"Failed to fetch data for {stock_symbol}. Status code: {response.status_code}\")\n\n    def setup_batches(self):\n        today = datetime.now()\n        while today.weekday() != 6:  # Find the latest sunday\n            today -= timedelta(days=1)\n        \n        # Generate the four weeks from today (Monday to Friday)\n        self.batches_to_check = []\n        for i in range(4):  # Generate 4 weeks\n            end_date = today - timedelta(weeks=i)\n            start_date = end_date - timedelta(days=6)  # Monday\n            self.batches_to_check.append((\n                start_date.replace(hour=0, minute=0, second=0), \n                end_date.replace(hour=23, minute=59, second=59)\n            ))\n        print(f\"Generated batches to check\")\n    \n    def chheck_stock_for_batch(self, symbol):   \n        newdf = self.avdata[self.avdata['stockname'] == symbol]\n        newdf['dates'] = pd.to_datetime(newdf['timestamp']).dt.date\n        available_dates = set(newdf['dates'])    \n        missing_weeks = []\n\n        for start_date, end_date in self.batches_to_check:\n            # Business days: Monday to Friday\n            week_dates = set(pd.date_range(start=start_date, end=end_date, freq='B').date)\n            \n            # Check if all the business days (Monday to Friday) are missing\n            if week_dates.isdisjoint(available_dates):\n                # print(f\"{symbol} Week {start_date} to {end_date} is entirely missing (all weekdays missing)\")\n                # Convert start_date and end_date to Unix timestamps\n                start_unix = int(start_date.timestamp())\n                end_unix = int(end_date.timestamp())\n                missing_weeks.append([symbol, start_unix, end_unix])\n        return missing_weeks\n\n    def detect_all_stocks(self):\n        all_stocks = self.avdata['stockname'].unique()\n        self.setup_batches()  \n        for st in tqdm(all_stocks):\n            missing_weeks = self.chheck_stock_for_batch(st)\n            self.recoverlist = self.recoverlist + missing_weeks\n        print(\"setup for targets completed\")\n        return self.recoverlist\n\n    def download_bunches_mass(self,targets):\n        print(\"starting scrappers\")\n        try:\n            for item in tqdm(targets):\n                self.collect_per_minute_data(item[0],item[1],item[2])\n        except Exception as error:\n            print(error)\n\n    def merge_new_data(self):\n        newframe = pd.DataFrame()\n        print(\"starting mergers\")\n        all_csvs = os.listdir('/kaggle/working/per_minute_recover/csv/')\n        for a_csv in tqdm(all_csvs):\n            all_files = os.listdir(f'/kaggle/working/per_minute_recover/csv/{a_csv}')\n            for a_file in all_files:\n                tmpdf = pd.read_csv(f'/kaggle/working/per_minute_recover/csv/{a_csv}/{a_file}')\n                tmpdf['stockname'] = a_csv.split('.')[0]\n                newframe = pd.concat([newframe,tmpdf],axis = 0)\n        print(\"finishing mergers\")\n        return newframe\n\n    def final_merge(self,newcollecteddf):\n        newdf = pd.concat([self.avdata,newcollecteddf])\n        del self.avdata\n        gc.collect()\n        print(\"prepared new dataframe\")\n        newdf = newdf[['stockname','timestamp','open','high','low','close','volume']]\n        newdf.to_csv('/kaggle/working/per_minute_recover/final/stocks.csv')\n        print(\"wrote new file\")\n        print(\"finishing main merge\")\n\n    def create_metadata_to_push_recover(self):\n        print('Creating metadata file for per minute data>>>>')\n        data = {\n            \"id\": \"ayushkhaire/real-time-stocks-data\"\n        }\n        metadata_file_location = '/kaggle/working/per_minute_recover/final/dataset-metadata.json' \n        with open(metadata_file_location, 'w', encoding='utf-8') as metadata_file:\n            json.dump(data, metadata_file)\n        print('Metadata file created for per minute data')\n\n    def upload_recovered_to_kaggle(self):\n        os.environ['KAGGLE_USERNAME'] = kaggle_username\n        os.environ['KAGGLE_KEY'] = kaggle_apikey\n        retries = 0\n        while retries < 5:\n            try:\n                command = \"kaggle datasets version -p '/kaggle/working/per_minute_recover/final' -m 'Update' -r zip\"\n                subprocess.run(command, shell=True, check=True)\n                logger.info(\"Upload completefor per minute data\")\n                break\n            except Exception as error:\n                logger.error(f\"Error from Kaggle: {error}\")\n                time.sleep(5)\n                retries += 1  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:45:45.435723Z","iopub.execute_input":"2025-02-18T07:45:45.436123Z","iopub.status.idle":"2025-02-18T07:45:45.464618Z","shell.execute_reply.started":"2025-02-18T07:45:45.436078Z","shell.execute_reply":"2025-02-18T07:45:45.463415Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ndf = RCC.avdata[RCC.avdata['stockname'] == \"PLTR\"]\n# ndf['d'] = ndf['timestamp'].str.split(\" \").str[0]  # Extract only the date part\n# print(ndf['d'].unique())  # Print unique dates for manual verification\n# ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:45:45.466556Z","iopub.execute_input":"2025-02-18T07:45:45.467015Z","iopub.status.idle":"2025-02-18T07:45:45.485981Z","shell.execute_reply.started":"2025-02-18T07:45:45.466968Z","shell.execute_reply":"2025-02-18T07:45:45.484617Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Driver code","metadata":{}},{"cell_type":"code","source":"# make force - True when notebook fails and do not update per minute data , and give saturdday data\nforce = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:45:45.487498Z","iopub.execute_input":"2025-02-18T07:45:45.487868Z","iopub.status.idle":"2025-02-18T07:45:45.502204Z","shell.execute_reply.started":"2025-02-18T07:45:45.487834Z","shell.execute_reply":"2025-02-18T07:45:45.500968Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"AC.delete(\"daily_data\",_del_all_ = True)\nAC.delete(\"per_minute_data\",_del_all_ = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:45:45.503743Z","iopub.execute_input":"2025-02-18T07:45:45.504456Z","iopub.status.idle":"2025-02-18T07:45:47.721282Z","shell.execute_reply.started":"2025-02-18T07:45:45.504403Z","shell.execute_reply":"2025-02-18T07:45:47.720069Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"STM = stocksManager()\nsymbols = STM.collect_stock_symbols()\nfinals_symbols = []\nfor n in symbols['names']:\n    for key in n.keys():\n        finals_symbols=finals_symbols+n[key]\nfinals_symbols = list(set(finals_symbols))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:45:47.722643Z","iopub.execute_input":"2025-02-18T07:45:47.723157Z","iopub.status.idle":"2025-02-18T07:46:09.929264Z","shell.execute_reply.started":"2025-02-18T07:45:47.723119Z","shell.execute_reply":"2025-02-18T07:46:09.928263Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 2/2 [00:01<00:00,  1.09it/s]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"STM.Kaggle_process_daily_data(finals_symbols)\ntoday = datetime.now()\nif today.weekday() == 0 or force == True:\n    print(\"there is monday today\")\n    yesterday = today - timedelta(days=1)\n    yesterdays_date = yesterday.strftime('%Y-%m-%d 00:00:00')\n    STM.Kaggle_process_per_minute_data(symbol_list = finals_symbols,last_date = yesterdays_date,weekday = 0)\nelse:\n    print(\"there is no monday today\")\nSTM.update_stocks_list_for_today()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:46:09.930293Z","iopub.execute_input":"2025-02-18T07:46:09.930619Z","iopub.status.idle":"2025-02-18T08:36:32.772833Z","shell.execute_reply.started":"2025-02-18T07:46:09.930588Z","shell.execute_reply":"2025-02-18T08:36:32.771186Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 2203/2203 [14:08<00:00,  2.60it/s]\n100%|██████████| 2197/2197 [07:15<00:00,  5.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"there is monday today\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2203/2203 [05:22<00:00,  6.82it/s]\n100%|██████████| 2198/2198 [05:06<00:00,  7.17it/s]\n100%|██████████| 2197/2197 [00:00<00:00, 1063092.51it/s]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# create metadata files","metadata":{}},{"cell_type":"code","source":"logging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n \nprint('Creating metadata file for daily data>>>>')\ndata = {\n    \"id\": \"ayushkhaire/stock-past-one-year-data\"\n}\nmetadata_file_location = '/kaggle/working/daily_update_to_kaggle_final/dataset-metadata.json' \nwith open(metadata_file_location, 'w', encoding='utf-8') as metadata_file:\n    json.dump(data, metadata_file)\nprint('Metadata file created for daily data')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:36:32.775451Z","iopub.execute_input":"2025-02-18T08:36:32.775853Z","iopub.status.idle":"2025-02-18T08:36:32.785116Z","shell.execute_reply.started":"2025-02-18T08:36:32.775806Z","shell.execute_reply":"2025-02-18T08:36:32.783852Z"}},"outputs":[{"name":"stdout","text":"Creating metadata file for daily data>>>>\nMetadata file created for daily data\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"if today.weekday() == 0 or force == True:\n    print('Creating metadata file for per minute data>>>>')\n    data = {\n        \"id\": \"ayushkhaire/real-time-stocks-data\"\n    }\n    metadata_file_location = '/kaggle/working/per_minute_to_kaggle_final/dataset-metadata.json' \n    with open(metadata_file_location, 'w', encoding='utf-8') as metadata_file:\n        json.dump(data, metadata_file)\n    print('Metadata file created for per minute data')\nelse:\n    print(\"there is no monday today\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:36:32.787231Z","iopub.execute_input":"2025-02-18T08:36:32.78775Z","iopub.status.idle":"2025-02-18T08:36:32.824385Z","shell.execute_reply.started":"2025-02-18T08:36:32.787694Z","shell.execute_reply":"2025-02-18T08:36:32.822916Z"}},"outputs":[{"name":"stdout","text":"Creating metadata file for per minute data>>>>\nMetadata file created for per minute data\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# upload","metadata":{}},{"cell_type":"code","source":"os.environ['KAGGLE_USERNAME'] = kaggle_username\nos.environ['KAGGLE_KEY'] = kaggle_apikey","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:36:32.828372Z","iopub.execute_input":"2025-02-18T08:36:32.828773Z","iopub.status.idle":"2025-02-18T08:36:32.839469Z","shell.execute_reply.started":"2025-02-18T08:36:32.82874Z","shell.execute_reply":"2025-02-18T08:36:32.83841Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"retries = 0\nwhile retries < 5:\n    try:\n        command = \"kaggle datasets version -p '/kaggle/working/daily_update_to_kaggle_final' -m 'Update' -r zip\"\n        subprocess.run(command, shell=True, check=True)\n        logger.info(\"Upload completefor daily data\")\n        break\n    except Exception as error:\n        logger.error(f\"Error from Kaggle: {error}\")\n        time.sleep(5)\n        retries += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:36:32.840947Z","iopub.execute_input":"2025-02-18T08:36:32.841546Z","iopub.status.idle":"2025-02-18T08:36:43.618178Z","shell.execute_reply.started":"2025-02-18T08:36:32.841491Z","shell.execute_reply":"2025-02-18T08:36:43.616299Z"}},"outputs":[{"name":"stdout","text":"Starting upload for file stocks.csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 527M/527M [00:08<00:00, 63.0MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Upload successful: stocks.csv (527MB)\nDataset version is being created. Please check progress at https://www.kaggle.com/ayushkhaire/stock-past-one-year-data\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"if today.weekday() == 0 or force == True:\n    print(\"there is monday today\")    \n    retries = 0\n    while retries < 5:\n        try:\n            command = \"kaggle datasets version -p '/kaggle/working/per_minute_to_kaggle_final' -m 'Update' -r zip\"\n            subprocess.run(command, shell=True, check=True)\n            logger.info(\"Upload completefor per minute data\")\n            break\n        except Exception as error:\n            logger.error(f\"Error from Kaggle: {error}\")\n            time.sleep(5)\n            retries += 1\nelse:\n    print(\"there is no saturday today\")    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:36:43.620916Z","iopub.execute_input":"2025-02-18T08:36:43.621581Z","iopub.status.idle":"2025-02-18T08:38:26.073737Z","shell.execute_reply.started":"2025-02-18T08:36:43.621496Z","shell.execute_reply":"2025-02-18T08:38:26.072439Z"}},"outputs":[{"name":"stdout","text":"there is monday today\nStarting upload for file stocks.csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6.68G/6.68G [01:40<00:00, 71.4MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: stocks.csv (7GB)\nDataset version is being created. Please check progress at https://www.kaggle.com/ayushkhaire/real-time-stocks-data\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# Recover lost one","metadata":{}},{"cell_type":"code","source":"RCC = recoverData()\nRCC.setup()\ntargets = RCC.detect_all_stocks()\ntargets = targets + STM.available_stocks\nRCC.download_bunches_mass(targets)\nnewcollecteddf = RCC.merge_new_data()\nRCC.final_merge(newcollecteddf)\nRCC.create_metadata_to_push_recover()\nRCC.upload_recovered_to_kaggle()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:38:26.07564Z","iopub.execute_input":"2025-02-18T08:38:26.075983Z","iopub.status.idle":"2025-02-18T09:58:35.044609Z","shell.execute_reply.started":"2025-02-18T08:38:26.075951Z","shell.execute_reply":"2025-02-18T09:58:35.04289Z"}},"outputs":[{"name":"stdout","text":"set up directories\nstock list and dataset loaded .\nGenerated batches to check\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 606/606 [1:01:14<00:00,  6.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"setup for targets completed\nstarting scrappers\n","output_type":"stream"},{"name":"stderr","text":" 21%|██▏       | 724/3397 [02:03<04:37,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"Error processing JSON data: 'timestamp'\nError processing JSON data: 'timestamp'\nError processing JSON data: 'timestamp'\nError processing JSON data: 'timestamp'\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▌      | 1198/3397 [03:16<04:13,  8.66it/s]","output_type":"stream"},{"name":"stdout","text":"Error processing JSON data: 'timestamp'\nError processing JSON data: 'timestamp'\nError processing JSON data: 'timestamp'\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▌      | 1200/3397 [03:16<03:28, 10.55it/s]","output_type":"stream"},{"name":"stdout","text":"Error processing JSON data: 'timestamp'\nFailed to fetch data for U. Status code: 400\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▌      | 1202/3397 [03:16<03:33, 10.27it/s]","output_type":"stream"},{"name":"stdout","text":"Failed to fetch data for C. Status code: 400\nFailed to fetch data for S. Status code: 400\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 1206/3397 [03:16<03:27, 10.58it/s]","output_type":"stream"},{"name":"stdout","text":"Failed to fetch data for E. Status code: 400\nFailed to fetch data for B. Status code: 400\nFailed to fetch data for B. Status code: 400\nFailed to fetch data for C. Status code: 400\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 1211/3397 [03:17<02:32, 14.37it/s]","output_type":"stream"},{"name":"stdout","text":"Failed to fetch data for A. Status code: 400\nFailed to fetch data for M. Status code: 400\nFailed to fetch data for E. Status code: 400\nFailed to fetch data for A. Status code: 400\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 1216/3397 [03:17<02:08, 16.99it/s]","output_type":"stream"},{"name":"stdout","text":"Failed to fetch data for L. Status code: 400\nFailed to fetch data for M. Status code: 400\nFailed to fetch data for P. Status code: 400\nFailed to fetch data for V. Status code: 400\nFailed to fetch data for M. Status code: 400\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 1220/3397 [03:17<02:03, 17.66it/s]","output_type":"stream"},{"name":"stdout","text":"Failed to fetch data for P. Status code: 400\nFailed to fetch data for Q. Status code: 400\nFailed to fetch data for L. Status code: 400\nFailed to fetch data for B. Status code: 400\nFailed to fetch data for P. Status code: 400\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 1225/3397 [03:17<01:52, 19.26it/s]","output_type":"stream"},{"name":"stdout","text":"Failed to fetch data for R. Status code: 400\nFailed to fetch data for D. Status code: 400\nFailed to fetch data for S. Status code: 400\nFailed to fetch data for W. Status code: 400\nFailed to fetch data for A. Status code: 400\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 1231/3397 [03:18<01:50, 19.68it/s]","output_type":"stream"},{"name":"stdout","text":"Failed to fetch data for F. Status code: 400\nFailed to fetch data for V. Status code: 400\nFailed to fetch data for P. Status code: 400\nFailed to fetch data for A. Status code: 400\nFailed to fetch data for A. Status code: 400\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▋      | 1234/3397 [03:18<01:47, 20.10it/s]","output_type":"stream"},{"name":"stdout","text":"Failed to fetch data for J. Status code: 400\nFailed to fetch data for V. Status code: 400\nFailed to fetch data for P. Status code: 400\nFailed to fetch data for O. Status code: 400\nFailed to fetch data for U. Status code: 400\n","output_type":"stream"},{"name":"stderr","text":" 37%|███▋      | 1240/3397 [03:18<01:46, 20.20it/s]","output_type":"stream"},{"name":"stdout","text":"Failed to fetch data for U. Status code: 400\nFailed to fetch data for P. Status code: 400\nFailed to fetch data for A. Status code: 400\nFailed to fetch data for P. Status code: 400\n","output_type":"stream"},{"name":"stderr","text":" 37%|███▋      | 1243/3397 [03:18<01:53, 18.90it/s]","output_type":"stream"},{"name":"stdout","text":"Failed to fetch data for T. Status code: 400\nFailed to fetch data for C. Status code: 400\nFailed to fetch data for O. Status code: 400\nFailed to fetch data for J. Status code: 400\nFailed to fetch data for V. Status code: 400\n","output_type":"stream"},{"name":"stderr","text":" 37%|███▋      | 1247/3397 [03:19<05:43,  6.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Failed to fetch data for T. Status code: 400\nFailed to fetch data for U. Status code: 400\nstring index out of range\nstarting mergers\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 298/298 [01:35<00:00,  3.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"finishing mergers\nprepared new dataframe\nwrote new file\nfinishing main merge\nCreating metadata file for per minute data>>>>\nMetadata file created for per minute data\nStarting upload for file stocks.csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6.56G/6.56G [01:38<00:00, 71.8MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: stocks.csv (7GB)\nDataset version is being created. Please check progress at https://www.kaggle.com/ayushkhaire/real-time-stocks-data\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"shutil.rmtree('/kaggle/working/daily_update')\nshutil.rmtree('/kaggle/working/daily_update_to_kaggle')\nif today.weekday() == 0 or force == True:\n    shutil.rmtree('/kaggle/working/per_minute')\n    shutil.rmtree('/kaggle/working/per_minute_to_kaggle')\n    shutil.rmtree('/kaggle/working/per_minute_recover')\n    shutil.rmtree('/kaggle/working/per_minute_to_kaggle_final')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T09:58:35.046567Z","iopub.execute_input":"2025-02-18T09:58:35.046959Z","iopub.status.idle":"2025-02-18T09:58:37.247037Z","shell.execute_reply.started":"2025-02-18T09:58:35.046918Z","shell.execute_reply":"2025-02-18T09:58:37.245968Z"}},"outputs":[],"execution_count":18}]}
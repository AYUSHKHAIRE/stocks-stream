{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":10995422,"datasetId":5902635,"databundleVersionId":11374663},{"sourceType":"datasetVersion","sourceId":10900661,"datasetId":4987536,"databundleVersionId":11269552}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ayushkhaire/real-time-data-update?scriptVersionId=227058481\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import requests as rq\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime\nimport time \nfrom tqdm import tqdm\nimport os\nfrom datetime import datetime,timedelta\nfrom pymongo.mongo_client import MongoClient\nfrom pymongo.server_api import ServerApi\nimport logging\nfrom kaggle_secrets import UserSecretsClient\nimport warnings\nimport json\nimport shutil\nimport subprocess\nimport gc\n\nwarnings.filterwarnings('ignore')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:08:11.01407Z","iopub.execute_input":"2025-03-11T14:08:11.014505Z","iopub.status.idle":"2025-03-11T14:08:12.367782Z","shell.execute_reply.started":"2025-03-11T14:08:11.014453Z","shell.execute_reply":"2025-03-11T14:08:12.366705Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(message)s\")\nlogger = logging.getLogger(__name__)\n\n\nlogger.info(\"This is an INFO message\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:08:16.604828Z","iopub.execute_input":"2025-03-11T14:08:16.605384Z","iopub.status.idle":"2025-03-11T14:08:16.610668Z","shell.execute_reply.started":"2025-03-11T14:08:16.605345Z","shell.execute_reply":"2025-03-11T14:08:16.60952Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Secrets","metadata":{}},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nkaggle_apikey = user_secrets.get_secret(\"kaggle_apikey\")\nkaggle_username = user_secrets.get_secret(\"kaggle_username\")\nmngodb_database_name = user_secrets.get_secret(\"mngodb_database_name\")\nmongodb_app_name = user_secrets.get_secret(\"mongodb_appname\")\nmongodb_password = user_secrets.get_secret(\"mongodb_password\")\nmongodb_username = user_secrets.get_secret(\"mongodb_username\")\nmongodb_cluster_name = user_secrets.get_secret(\"mongodb_cluster_name\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:08:18.785762Z","iopub.execute_input":"2025-03-11T14:08:18.786125Z","iopub.status.idle":"2025-03-11T14:08:20.088541Z","shell.execute_reply.started":"2025-03-11T14:08:18.786092Z","shell.execute_reply":"2025-03-11T14:08:20.087409Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# MongoDB","metadata":{}},{"cell_type":"code","source":"class AtlasClient:\n    def __init__(self, atlas_uri, dbname):\n        self.mongodb_client = MongoClient(atlas_uri)\n        self.database = self.mongodb_client[dbname]\n\n    def ping(self):\n        try:\n            self.mongodb_client.admin.command('ping')\n            logging.info(\"Pinged your MongoDB deployment. Connection successful.\")\n        except Exception as e:\n            logging.error(f\"Failed to connect to MongoDB: {e}\")\n\n    def get_collection(self, collection_name):\n        collection = self.database[collection_name]\n        return collection\n\n    def findOneByKey(self,collection_name,key):\n        collection = self.get_collection(collection_name)\n        result = collection.find_one({ key: { \"$exists\": True } })\n        return result\n\n\n    def find(self, collection_name, filter={}, limit=0):\n        collection = self.database[collection_name]\n        items = list(collection.find(filter=filter, limit=limit))\n        return items\n    \n    def insert(self, collection_name, documents):\n        \"\"\"\n        Inserts one or more documents into a MongoDB collection.\n        \n        Parameters:\n        - collection_name: str, the name of the collection\n        - documents: dict or list of dicts, the document(s) to insert\n        \n        If `documents` is a list, it will insert multiple documents using `insert_many`.\n        Otherwise, it will insert a single document using `insert_one`.\n        \"\"\"\n        collection = self.get_collection(collection_name)\n        \n        if isinstance(documents, list):\n            result = collection.insert_many(documents)\n            return result.inserted_ids\n        else:\n            result = collection.insert_one(documents)\n            return result.inserted_id\n        \n    def delete(self, collection_name, filter={}, _del_all_=False):\n        \"\"\"\n        Deletes documents from a MongoDB collection based on the filter.\n        \n        Parameters:\n        - collection_name: str, the name of the collection.\n        - filter: dict, the filter to find documents to delete (default is {}).\n        - _del_all_: bool, if True, deletes all documents matching the filter using `delete_many()`.\n                      If False, deletes only one document using `delete_one()`.\n        \n        Returns:\n        - Number of documents deleted.\n        \"\"\"\n        collection = self.get_collection(collection_name)\n        \n        if _del_all_:\n            result = collection.delete_many(filter)\n            return result.deleted_count\n        else:\n            result = collection.delete_one(filter)\n            if result.deleted_count == 1:\n                pass\n            else:\n                pass\n            return result.deleted_count","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:08:22.17137Z","iopub.execute_input":"2025-03-11T14:08:22.171997Z","iopub.status.idle":"2025-03-11T14:08:22.18881Z","shell.execute_reply.started":"2025-03-11T14:08:22.171944Z","shell.execute_reply":"2025-03-11T14:08:22.187678Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Stocks manager","metadata":{}},{"cell_type":"code","source":"AC = AtlasClient(\n    atlas_uri=f\"mongodb+srv://{mongodb_username}:{mongodb_password}@{mongodb_cluster_name}.fznbh.mongodb.net/?retryWrites=true&w=majority&appName={mongodb_app_name}\",\n    dbname = mngodb_database_name\n)\n\n\nclass stocksManager:\n    def __init__(self) -> None:\n        self.available_stocks = []\n        headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n        }\n        self.headers = headers\n        self.firstrun = 0\n\n    def collect_stock_symbols(self):\n        targets = [\n            '52-week-gainers', \n            '52-week-losers'\n        ]   \n    \n        limitlist = []\n\n        for page in tqdm(targets):\n            url = f'https://finance.yahoo.com/markets/stocks/{page}/?start=0&count=100'\n            # print(url)\n            try:\n                r = rq.get(url,headers = self.headers)\n            except Exception as e:\n                logger.warning(\"cannot hit url : \",url ,e,r.status_code)\n            soup = BeautifulSoup(r.text,'html.parser')\n            limits = soup.find(\n                'div',{'class':'total'}\n            ).text\n            limits = limits.split(' ')[2]\n            limitlist.append(limits)\n\n        max_hits = []\n        for limit in limitlist:\n            max_hit = int(int(limit) / 100)\n            max_hits.append(max_hit)\n\n        findict = {\n            'targets':targets,\n            'max_hits':max_hits\n        }\n        \n        urls_for_stocks = []\n\n        i = 0\n        for i in range(\n            len(\n                findict['targets']\n                )\n            ):\n            target = findict['targets'][i]\n            maxhit = findict['max_hits'][i]\n            for m in range(maxhit+1):\n                url = f'https://finance.yahoo.com/markets/stocks/{target}/?start={m*100}&count=100/'\n                urls_for_stocks.append(url)\n\n        data = []\n\n        logger.info('collecting data for symbols _______________________________--')\n        for u in urls_for_stocks:\n            catg = u.split('/')[-3]\n            symbol_list = []\n            try:\n                r = rq.get(u,headers = self.headers)\n            except Exception as e:\n                logger.warning(\"cannot hit url : \",u ,r.status_code)\n            soup = BeautifulSoup(r.text,'html.parser')\n            symbs= soup.find_all('span',{'class':'symbol'})\n            for s in symbs:\n                symbol_list.append(s.text)\n            data.append(\n                {catg:symbol_list}\n            )\n        logger.info(\"finished collecting data for symbols ______________________________-\")\n        data = {'names':data}\n        return data\n    \n    def return_list_for_symbols(self):\n        symbols = self.collect_stock_symbols()\n        finals_symbols = []\n        for n in symbols['names']:\n            for key in n.keys():\n                finals_symbols=finals_symbols+n[key]\n        finals_symbols = list(set(finals_symbols))\n        return finals_symbols\n\n    def return_human_timestamp(self, timestamps):\n            if isinstance(timestamps, list):\n                new_dates = []\n                for unix_time in timestamps:\n                    try:\n                        if isinstance(unix_time, str):\n                            datetime.strptime(unix_time, '%Y-%m-%d %H:%M:%S') \n                            new_dates.append(unix_time)\n                        else:\n                            unix_time = float(unix_time)\n                            date = datetime.fromtimestamp(unix_time).strftime('%Y-%m-%d %H:%M:%S')\n                            new_dates.append(date)\n                    except (ValueError, TypeError):\n                        new_dates.append(None)  \n                return new_dates\n            elif isinstance(timestamps, str):\n                try:\n                    unix_time = float(timestamps)\n                    date = datetime.fromtimestamp(unix_time).strftime('%Y-%m-%d %H:%M:%S')\n                    return date\n                except (ValueError, TypeError):\n                    return None\n\n    def return_unix_timestamps(self, date_strings):\n        if isinstance(date_strings, list):\n            unix_timestamps = []\n            for date_str in date_strings:\n                try:\n                    dt = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')\n                    unix_timestamp = int(dt.timestamp())  \n                    unix_timestamps.append(unix_timestamp)\n                except (ValueError, TypeError):\n                    unix_timestamps.append(None)\n            return unix_timestamps\n        elif isinstance(date_strings, str):\n            try:\n                dt = datetime.strptime(date_strings, '%Y-%m-%d %H:%M:%S')\n                unix_timestamp = int(dt.timestamp())  \n                return unix_timestamp\n            except (ValueError, TypeError):\n                return None\n\n    def update_prices_for_daily(self, symbol_list):\n        current_timestamp = int(time.time())\n        current_time = datetime.fromtimestamp(current_timestamp)\n        human_readable_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n        \n        # Start and end periods for data retrieval\n        start_date_str = \"2015-01-01\"\n        start_date_obj = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n        period1 = int(time.mktime(start_date_obj.timetuple()))\n        period2 = current_timestamp\n        \n        logger.warning(f\"Daily data for today's date {human_readable_time}\")\n        logger.info(f\"Checking updates for period1={period1} & period2={period2} for stocks daily\")\n\n        # Define base path for daily updates\n        files_path = f'/kaggle/working/daily_update/'\n        os.makedirs('/kaggle/working/daily_update/',exist_ok = True)\n        os.makedirs('/kaggle/working/daily_update_to_kaggle/',exist_ok = True)\n        AC.delete(\n            \"daily_data\",\n            _del_all_ = True\n        )\n        for stock in tqdm(symbol_list):\n            stock_symbol = stock.replace(' ', '')\n            json_path = f'{files_path}/{stock_symbol}.json'\n            os.makedirs(os.path.dirname(json_path), exist_ok=True)\n            \n            url = (f'https://query1.finance.yahoo.com/v8/finance/chart/{stock_symbol}?events=capitalGain%7Cdiv%7Csplit'\n                f'&formatted=true&includeAdjustedClose=true&interval=1d&period1={period1}&period2={period2}'\n                f'&symbol={stock_symbol}&userYfid=true&lang=en-US&region=US')\n            try:\n                response = rq.get(url, headers=self.headers)\n                if response.status_code == 200:\n                    with open(json_path, 'wb') as file:\n                        file.write(response.content)\n                    json_data = pd.read_json(json_path)\n                    timestamp = json_data['chart']['result'][0].get('timestamp')\n                    if timestamp:\n                        new_timestamps = self.return_human_timestamp(timestamp)\n                        new_data = json_data['chart']['result'][0]['indicators']['quote'][0]\n                        new_data['timestamp'] = new_timestamps\n                        data_to_insert = {f'{stock_symbol}':new_data}\n                        if data_to_insert:\n#                             in database\n                            AC.insert(\n                                collection_name=\"daily_data\",\n                                documents=data_to_insert\n                            )\n#                            local\n                            new_data = pd.DataFrame(new_data)\n                            new_data.to_csv(f'/kaggle/working/daily_update_to_kaggle/{stock}.csv')\n                            \n                        else:\n                            logger.error(f'daily data insertion for {stock} failed .',e)\n                else:\n                    logger.warning(f\"Request failed: {url}, Status code: {response.status_code}\")\n                    continue\n            except:\n                continue\n        logger.info(\"Daily data update finished.\")\n  \n    \n    def update_prices_for_per_minute(self, symbol_list,last_date):\n        os.makedirs(f'/kaggle/working/per_minute/', exist_ok=True)\n        os.makedirs(f'/kaggle/working/per_minute_to_kaggle/', exist_ok=True) \n        date_time_obj = datetime.strptime(last_date, '%Y-%m-%d %H:%M:%S')\n        period1 = int(date_time_obj.timestamp())\n        seven_days_back = date_time_obj - timedelta(days=7)\n        period2 = int(seven_days_back.timestamp())\n  \n        logger.info(f\"Checking updates for period1={period1} & period2={period2} for stocks per minute.\")\n            \n        AC.delete(\n                collection_name=\"per_minute_data\",\n                _del_all_ = True\n        )\n        if symbol_list:\n            for stock in tqdm(symbol_list):\n                try:\n                    stock_symbol = stock.replace(' ', '')\n                    link = f'https://query2.finance.yahoo.com/v8/finance/chart/{stock_symbol}?period1={period2}&period2={period1}&interval=1m&includePrePost=true&events=div%7Csplit%7Cearn&&lang=en-US&region=US'\n                    response = rq.get(link, headers=self.headers)\n                    tmppath = f'/kaggle/working/per_minute/{stock_symbol}.json'\n                    if response.status_code == 200:\n                        with open(tmppath, 'wb') as jsn:\n                            jsn.write(response.content)\n                        json_data  = pd.read_json(tmppath)\n                        timestamp = json_data['chart'][0][0]['timestamp']\n                        json_data = json_data['chart'][0][0][\"indicators\"][\"quote\"][0]\n                        try:\n                            new_timestamps = self.return_human_timestamp(timestamp)\n                            json_data['timestamp'] = new_timestamps\n                            data_to_insert = {f'{stock_symbol}':json_data}\n    #                         to database\n                            if data_to_insert:\n                            #    AC.insert(\n                            #         collection_name=\"per_minute_data\",\n                            #         documents=data_to_insert\n                            #    )\n    #                           to csv\n                                json_data = pd.DataFrame(json_data)\n                                json_data.to_csv(f'/kaggle/working/per_minute_to_kaggle/{stock}.csv')\n                            else:\n                                logger.warning(f'per minute data insertion data insertion for {stock} failed .'),e\n\n                        except Exception as e:\n                            logger.warning(f\"Request failed: {link}, Status code: {response.status_code}\")\n                            print('failed',e)\n                            continue\n                except:\n                    continue\n                \n        else:\n            logger.warning(\"It is not Sunday today. Skipping the update step.\")\n        logger.info(\"Per minute update finished.\")\n\n    def update_stocks_list_for_today(self):\n        AC.delete('master',_del_all_ = True)\n        stocks = AC.find(\"daily_data\")\n        self.kaggle_process_per_minute_data  \n        stockslist = []\n        for st in tqdm(stocks):\n            stockslist.append(list(st.keys())[1])\n        self.available_stocks = stockslist\n        AC.insert(\"master\",{'stocks':stockslist})\n        logger.warning(\"stocks list updated !\")\n        \n    # specific to kaggle\n    def Kaggle_process_daily_data(self,symbol_list):\n        self.update_prices_for_daily(symbol_list)\n        megadatadailyframe = pd.DataFrame()\n        daily_files_csv = os.listdir('/kaggle/working/daily_update_to_kaggle')\n\n        for csv in tqdm(daily_files_csv):\n            df = pd.read_csv(f'/kaggle/working/daily_update_to_kaggle/{csv}')\n            df['stockname'] = csv.split('.')[0]\n            megadatadailyframe = pd.concat([megadatadailyframe,df],axis = 0)\n            \n        os.makedirs(f'/kaggle/working/daily_update_to_kaggle_final',exist_ok = True)\n        megadatadailyframe.to_csv('/kaggle/working/daily_update_to_kaggle_final/stocks.csv')\n    \n    def kaggle_process_per_minute_data(self,symbol_list,last_date,weekday):\n        self.update_prices_for_per_minute(symbol_list,last_date)\n        perminute_files_csv = os.listdir('/kaggle/working/per_minute_to_kaggle/')\n        existing_data = pd.read_csv('/kaggle/input/real-time-stocks-data/stocks.csv')\n        os.makedirs(f'/kaggle/working/per_minute_to_kaggle_final',exist_ok = True)\n        for csv in tqdm(perminute_files_csv):\n            df = pd.read_csv(f'/kaggle/working/per_minute_to_kaggle/{csv}')\n            df['stockname'] = csv.split('.')[0]\n            org_data = existing_data[existing_data['stockname'] == csv.split()[0].replace(' ','')]\n            new_data = pd.concat([org_data,df],axis = 0)\n            # print(len(df),len(org_data),len(new_data))\n            new_data = new_data[['stockname','timestamp','open','high','low','close']]\n            new_data = new_data.drop_duplicates()\n            new_data.to_csv(f'/kaggle/working/per_minute_to_kaggle_final/{csv}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:57:57.693266Z","iopub.execute_input":"2025-03-11T14:57:57.693779Z","iopub.status.idle":"2025-03-11T14:57:57.772517Z","shell.execute_reply.started":"2025-03-11T14:57:57.693736Z","shell.execute_reply":"2025-03-11T14:57:57.771283Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"# Recover data if any lost","metadata":{}},{"cell_type":"code","source":"class recoverData:\n    def __init__(self):\n        self.symbol_list = None\n        self.avdata = None\n        self.headers = {'User-Agent': 'Mozilla/5.0'}\n        self.batches_to_check = []\n        self.recoverlist = []\n\n    def setup(self):\n        os.makedirs(\"/kaggle/working/per_minute_recover/json/\", exist_ok=True)\n        os.makedirs(\"/kaggle/working/per_minute_recover/csv/\", exist_ok=True)\n        os.makedirs(\"/kaggle/working/per_minute_recover/final/\", exist_ok=True)\n        print(\"set up directories\")\n        self.avdata = pd.read_csv('/kaggle/input/real-time-stocks-data/stocks.csv')\n        self.symbol_list = self.avdata['stockname'].unique()\n        print(\"stock list and dataset loaded .\")\n\n    def return_unix_timestamps(self, date_strings):\n        try:\n            dt = datetime.strptime(date_strings, '%Y-%m-%d')\n            return int(dt.timestamp())\n        except ValueError:\n            return None\n\n    def return_human_timestamp(self, timestamps):\n        try:\n            return [datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S') for ts in timestamps]\n        except Exception:\n            return None\n\n    def collect_per_minute_data(self, stock_symbol, start_date, end_date):\n        period1 = start_date\n        period2 = end_date\n        if period1 is None or period2 is None:\n            print(f\"Invalid dates provided: {start_date}, {end_date}\")\n            return\n\n        stock_symbol = stock_symbol.replace(' ', '')\n        link = f\"https://query2.finance.yahoo.com/v8/finance/chart/{stock_symbol}?period1={period1}&period2={period2}&interval=1m&includePrePost=true&events=div%7Csplit%7Cearnings&lang=en-US&region=US\"\n        response = rq.get(link, headers=self.headers)\n\n        if response.status_code == 200:\n            tmppath = f'/kaggle/working/per_minute_recover/json/{stock_symbol}.json'\n            with open(tmppath, 'wb') as jsn:\n                jsn.write(response.content)\n            json_data = response.json()\n\n            try:\n                timestamps = json_data['chart']['result'][0]['timestamp']\n                indicators = json_data['chart']['result'][0]['indicators']['quote'][0]\n                indicators['timestamp'] = self.return_human_timestamp(timestamps)\n\n                df = pd.DataFrame(indicators)\n                os.makedirs(f'/kaggle/working/per_minute_recover/csv/{stock_symbol}/',exist_ok = True)\n                df.to_csv(\n                    f\"/kaggle/working/per_minute_recover/csv/{stock_symbol}/{start_date}_{end_date}.csv\",\n                    index=False,\n                )\n            except KeyError as e:\n                print(f\"Error processing JSON data: {e}\")\n        else:\n            print(f\"Failed to fetch data for {stock_symbol}. Status code: {response.status_code}\")\n\n    def setup_batches(self):\n        today = datetime.now()\n        while today.weekday() != 6:  # Find the latest sunday\n            today -= timedelta(days=1)\n        \n        # Generate the four weeks from today (Monday to Friday)\n        self.batches_to_check = []\n        for i in range(4):  # Generate 4 weeks\n            end_date = today - timedelta(weeks=i)\n            start_date = end_date - timedelta(days=6)  # Monday\n            self.batches_to_check.append((\n                start_date.replace(hour=0, minute=0, second=0), \n                end_date.replace(hour=23, minute=59, second=59)\n            ))\n        print(f\"Generated batches to check\")\n    \n    def chheck_stock_for_batch(self, symbol):   \n        newdf = self.avdata[self.avdata['stockname'] == symbol]\n        newdf['dates'] = pd.to_datetime(newdf['timestamp']).dt.date\n        available_dates = set(newdf['dates'])    \n        missing_weeks = []\n\n        for start_date, end_date in self.batches_to_check:\n            # Business days: Monday to Friday\n            week_dates = set(pd.date_range(start=start_date, end=end_date, freq='B').date)\n            \n            # Check if all the business days (Monday to Friday) are missing\n            if week_dates.isdisjoint(available_dates):\n                # print(f\"{symbol} Week {start_date} to {end_date} is entirely missing (all weekdays missing)\")\n                # Convert start_date and end_date to Unix timestamps\n                start_unix = int(start_date.timestamp())\n                end_unix = int(end_date.timestamp())\n                missing_weeks.append([symbol, start_unix, end_unix])\n        return missing_weeks\n\n    def detect_all_stocks(self):\n        all_stocks = self.avdata['stockname'].unique()\n        self.setup_batches()  \n        for st in tqdm(all_stocks):\n            missing_weeks = self.chheck_stock_for_batch(st)\n            self.recoverlist = self.recoverlist + missing_weeks\n        print(\"setup for targets completed\")\n        return self.recoverlist\n\n    def download_bunches_mass(self,targets):\n        print(\"starting scrappers\")\n        try:\n            for item in tqdm(targets):\n                self.collect_per_minute_data(item[0],item[1],item[2])\n        except Exception as error:\n            print(error)\n\n    def merge_new_data(self):\n        newframe = pd.DataFrame()\n        print(\"starting mergers\")\n        all_csvs = os.listdir('/kaggle/working/per_minute_recover/csv/')\n        for a_csv in tqdm(all_csvs):\n            all_files = os.listdir(f'/kaggle/working/per_minute_recover/csv/{a_csv}')\n            for a_file in all_files:\n                tmpdf = pd.read_csv(f'/kaggle/working/per_minute_recover/csv/{a_csv}/{a_file}')\n                tmpdf['stockname'] = a_csv.split('.')[0]\n                newframe = pd.concat([newframe,tmpdf],axis = 0)\n        print(\"finishing mergers\")\n        return newframe\n\n    def final_merge(self,newcollecteddf):\n        newdf = pd.concat([self.avdata,newcollecteddf])\n        del self.avdata\n        gc.collect()\n        print(\"prepared new dataframe\")\n        newdf = newdf[['stockname','timestamp','open','high','low','close','volume']]\n        newdf.to_csv('/kaggle/working/per_minute_recover/final/stocks.csv')\n        print(\"wrote new file\")\n        print(\"finishing main merge\")\n\n    def create_metadata_to_push_recover(self):\n        print('Creating metadata file for per minute data>>>>')\n        data = {\n            \"id\": \"ayushkhaire/real-time-stocks-data\"\n        }\n        metadata_file_location = '/kaggle/working/per_minute_recover/final/dataset-metadata.json' \n        with open(metadata_file_location, 'w', encoding='utf-8') as metadata_file:\n            json.dump(data, metadata_file)\n        print('Metadata file created for per minute data')\n\n    def upload_recovered_to_kaggle(self):\n        os.environ['KAGGLE_USERNAME'] = kaggle_username\n        os.environ['KAGGLE_KEY'] = kaggle_apikey\n        retries = 0\n        while retries < 5:\n            try:\n                command = \"kaggle datasets version -p '/kaggle/working/per_minute_recover/final' -m 'Update' -r zip\"\n                subprocess.run(command, shell=True, check=True)\n                logger.info(\"Upload completefor per minute data\")\n                break\n            except Exception as error:\n                logger.error(f\"Error from Kaggle: {error}\")\n                time.sleep(5)\n                retries += 1  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:45:45.435723Z","iopub.execute_input":"2025-02-18T07:45:45.436123Z","iopub.status.idle":"2025-02-18T07:45:45.464618Z","shell.execute_reply.started":"2025-02-18T07:45:45.436078Z","shell.execute_reply":"2025-02-18T07:45:45.463415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ndf = RCC.avdata[RCC.avdata['stockname'] == \"PLTR\"]\n# ndf['d'] = ndf['timestamp'].str.split(\" \").str[0]  # Extract only the date part\n# print(ndf['d'].unique())  # Print unique dates for manual verification\n# ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:45:45.466556Z","iopub.execute_input":"2025-02-18T07:45:45.467015Z","iopub.status.idle":"2025-02-18T07:45:45.485981Z","shell.execute_reply.started":"2025-02-18T07:45:45.466968Z","shell.execute_reply":"2025-02-18T07:45:45.484617Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Driver code","metadata":{}},{"cell_type":"code","source":"# make force - True when notebook fails and do not update per minute data , and give saturdday data\nforce = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:09:56.850758Z","iopub.execute_input":"2025-03-11T14:09:56.851191Z","iopub.status.idle":"2025-03-11T14:09:56.857525Z","shell.execute_reply.started":"2025-03-11T14:09:56.851145Z","shell.execute_reply":"2025-03-11T14:09:56.855667Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"AC.delete(\"daily_data\",_del_all_ = True)\nAC.delete(\"per_minute_data\",_del_all_ = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:09:23.987738Z","iopub.execute_input":"2025-03-11T14:09:23.988125Z","iopub.status.idle":"2025-03-11T14:09:25.125482Z","shell.execute_reply.started":"2025-03-11T14:09:23.988084Z","shell.execute_reply":"2025-03-11T14:09:25.124159Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"STM = stocksManager()\nsymbols = STM.collect_stock_symbols()\nfinals_symbols = []\nfor n in symbols['names']:\n    for key in n.keys():\n        finals_symbols=finals_symbols+n[key]\nfinals_symbols = list(set(finals_symbols))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:58:05.02203Z","iopub.execute_input":"2025-03-11T14:58:05.022494Z","iopub.status.idle":"2025-03-11T14:58:24.839787Z","shell.execute_reply.started":"2025-03-11T14:58:05.022431Z","shell.execute_reply":"2025-03-11T14:58:24.838786Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 2/2 [00:01<00:00,  1.18it/s]\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"# STM.Kaggle_process_daily_data(finals_symbols)\ntoday = datetime.now()\nif today.weekday() == 0 or force == True:\n    print(\"there is monday today\")\n    yesterday = today - timedelta(days=1)\n    yesterdays_date = yesterday.strftime('%Y-%m-%d 00:00:00')\n    STM.kaggle_process_per_minute_data(symbol_list = finals_symbols,last_date = yesterdays_date,weekday = 0)\nelse:\n    print(\"there is no monday today\")\nSTM.update_stocks_list_for_today()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:58:24.841528Z","iopub.execute_input":"2025-03-11T14:58:24.841837Z","execution_failed":"2025-03-11T18:11:50.665Z"}},"outputs":[{"name":"stdout","text":"there is monday today\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1990/1990 [05:20<00:00,  6.21it/s]\n 39%|███▉      | 775/1985 [1:47:44<2:46:19,  8.25s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# create metadata files","metadata":{}},{"cell_type":"code","source":"logging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n \nprint('Creating metadata file for daily data>>>>')\ndata = {\n    \"id\": \"ayushkhaire/stock-past-one-year-data\"\n}\nmetadata_file_location = '/kaggle/working/daily_update_to_kaggle_final/dataset-metadata.json' \nwith open(metadata_file_location, 'w', encoding='utf-8') as metadata_file:\n    json.dump(data, metadata_file)\nprint('Metadata file created for daily data')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:36:32.775451Z","iopub.execute_input":"2025-02-18T08:36:32.775853Z","iopub.status.idle":"2025-02-18T08:36:32.785116Z","shell.execute_reply.started":"2025-02-18T08:36:32.775806Z","shell.execute_reply":"2025-02-18T08:36:32.783852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if today.weekday() == 0 or force == True:\n    print('Creating metadata file for per minute data>>>>')\n    data = {\n        \"id\": \"ayushkhaire/real-time-stocks-data\"\n    }\n    metadata_file_location = '/kaggle/working/per_minute_to_kaggle_final/dataset-metadata.json' \n    with open(metadata_file_location, 'w', encoding='utf-8') as metadata_file:\n        json.dump(data, metadata_file)\n    print('Metadata file created for per minute data')\nelse:\n    print(\"there is no monday today\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:21:24.56325Z","iopub.execute_input":"2025-03-11T14:21:24.563666Z","iopub.status.idle":"2025-03-11T14:21:24.5717Z","shell.execute_reply.started":"2025-03-11T14:21:24.563629Z","shell.execute_reply":"2025-03-11T14:21:24.570515Z"}},"outputs":[{"name":"stdout","text":"Creating metadata file for per minute data>>>>\nMetadata file created for per minute data\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# upload","metadata":{}},{"cell_type":"code","source":"os.environ['KAGGLE_USERNAME'] = kaggle_username\nos.environ['KAGGLE_KEY'] = kaggle_apikey","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:21:28.111646Z","iopub.execute_input":"2025-03-11T14:21:28.112601Z","iopub.status.idle":"2025-03-11T14:21:28.11727Z","shell.execute_reply.started":"2025-03-11T14:21:28.112563Z","shell.execute_reply":"2025-03-11T14:21:28.116167Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"retries = 0\nwhile retries < 5:\n    try:\n        command = \"kaggle datasets version -p '/kaggle/working/daily_update_to_kaggle_final' -m 'Update' -r zip\"\n        subprocess.run(command, shell=True, check=True)\n        logger.info(\"Upload completefor daily data\")\n        break\n    except Exception as error:\n        logger.error(f\"Error from Kaggle: {error}\")\n        time.sleep(5)\n        retries += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:36:32.840947Z","iopub.execute_input":"2025-02-18T08:36:32.841546Z","iopub.status.idle":"2025-02-18T08:36:43.618178Z","shell.execute_reply.started":"2025-02-18T08:36:32.841491Z","shell.execute_reply":"2025-02-18T08:36:43.616299Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if today.weekday() == 0 or force == True:\n    print(\"there is monday today\")    \n    retries = 0\n    while retries < 5:\n        try:\n            command = \"kaggle datasets version -p '/kaggle/working/per_minute_to_kaggle_final' -m 'Update' -r zip\"\n            subprocess.run(command, shell=True, check=True)\n            logger.info(\"Upload completefor per minute data\")\n            break\n        except Exception as error:\n            logger.error(f\"Error from Kaggle: {error}\")\n            time.sleep(5)\n            retries += 1\nelse:\n    print(\"there is no saturday today\")    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:55:18.132094Z","iopub.execute_input":"2025-03-11T14:55:18.132584Z","iopub.status.idle":"2025-03-11T14:55:50.766863Z","shell.execute_reply.started":"2025-03-11T14:55:18.13254Z","shell.execute_reply":"2025-03-11T14:55:50.765699Z"}},"outputs":[{"name":"stdout","text":"there is monday today\nStarting upload for file LEN .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 221k/221k [00:00<00:00, 293kB/s]\n  0%|          | 0.00/172k [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: LEN .csv (221KB)\nStarting upload for file RARE .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 172k/172k [00:00<00:00, 300kB/s]\n  0%|          | 0.00/217k [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: RARE .csv (172KB)\nStarting upload for file BILL .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 217k/217k [00:00<00:00, 363kB/s]\n  0%|          | 0.00/105k [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: BILL .csv (217KB)\nStarting upload for file VMI .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 105k/105k [00:00<00:00, 213kB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: VMI .csv (105KB)\nStarting upload for file PONY .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2.03M/2.03M [00:01<00:00, 1.91MB/s]\n  0%|          | 0.00/2.35M [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: PONY .csv (2MB)\nStarting upload for file KVUE .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2.35M/2.35M [00:01<00:00, 2.39MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: KVUE .csv (2MB)\nStarting upload for file DLR .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 213k/213k [00:00<00:00, 356kB/s]\n  0%|          | 0.00/195k [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: DLR .csv (213KB)\nStarting upload for file SLM .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 195k/195k [00:00<00:00, 333kB/s]\n  0%|          | 0.00/223k [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: SLM .csv (195KB)\nStarting upload for file TEAM .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 223k/223k [00:00<00:00, 348kB/s]\n  0%|          | 0.00/203k [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: TEAM .csv (223KB)\nStarting upload for file ZM .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 203k/203k [00:00<00:00, 338kB/s]\n  0%|          | 0.00/188k [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: ZM .csv (203KB)\nStarting upload for file CMI .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 188k/188k [00:00<00:00, 311kB/s]\n  0%|          | 0.00/143k [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: CMI .csv (188KB)\nStarting upload for file AGYS .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 143k/143k [00:00<00:00, 254kB/s]\n  0%|          | 0.00/165k [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: AGYS .csv (143KB)\nStarting upload for file TIMB .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 165k/165k [00:00<00:00, 284kB/s]\n  0%|          | 0.00/205k [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: TIMB .csv (165KB)\nStarting upload for file ALL .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 205k/205k [00:00<00:00, 338kB/s]\n  0%|          | 0.00/197k [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: ALL .csv (205KB)\nStarting upload for file BXSL .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 197k/197k [00:00<00:00, 331kB/s]\n  0%|          | 0.00/200k [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: BXSL .csv (197KB)\nStarting upload for file EXEL .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 200k/200k [00:00<00:00, 346kB/s]\n  0%|          | 0.00/2.25M [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: EXEL .csv (200KB)\nStarting upload for file ADT .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2.25M/2.25M [00:01<00:00, 2.07MB/s]\n  0%|          | 0.00/192k [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: ADT .csv (2MB)\nStarting upload for file VNOM .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 192k/192k [00:00<00:00, 323kB/s]\n  0%|          | 0.00/203k [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: VNOM .csv (192KB)\nStarting upload for file CTSH .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 203k/203k [00:00<00:00, 335kB/s]\n  0%|          | 0.00/86.2k [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: CTSH .csv (203KB)\nStarting upload for file BAC-PK .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 86.2k/86.2k [00:00<00:00, 211kB/s]\n  0%|          | 0.00/224k [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: BAC-PK .csv (86KB)\nStarting upload for file LUV.csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 224k/224k [00:00<00:00, 335kB/s]\n  0%|          | 0.00/206k [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: LUV.csv (224KB)\nStarting upload for file AVTR .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 206k/206k [00:00<00:00, 345kB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: AVTR .csv (206KB)\nStarting upload for file OSCR .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2.67M/2.67M [00:01<00:00, 2.56MB/s]\n  0%|          | 0.00/56.2k [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: OSCR .csv (3MB)\nStarting upload for file GNGYF .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 56.2k/56.2k [00:00<00:00, 165kB/s]\n  0%|          | 0.00/154k [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: GNGYF .csv (56KB)\nStarting upload for file BWIN .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 154k/154k [00:00<00:00, 271kB/s]\n  0%|          | 0.00/199k [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: BWIN .csv (154KB)\nStarting upload for file MGY .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 199k/199k [00:00<00:00, 328kB/s]\n  0%|          | 0.00/187k [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: MGY .csv (199KB)\nStarting upload for file ALV .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 187k/187k [00:00<00:00, 314kB/s]\n  0%|          | 0.00/187k [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: ALV .csv (187KB)\nStarting upload for file J .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 187k/187k [00:00<00:00, 322kB/s]\n  0%|          | 0.00/3.18M [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: J .csv (187KB)\nStarting upload for file AI .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3.18M/3.18M [00:01<00:00, 2.83MB/s]\n  0%|          | 0.00/217k [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: AI .csv (3MB)\nStarting upload for file KEYS .csv\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 217k/217k [00:00<00:00, 347kB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: KEYS .csv (217KB)\nDataset version is being created. Please check progress at https://www.kaggle.com/ayushkhaire/real-time-stocks-data\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"# Recover lost one","metadata":{}},{"cell_type":"code","source":"RCC = recoverData()\nRCC.setup()\ntargets = RCC.detect_all_stocks()\ntargets = targets + STM.available_stocks\nRCC.download_bunches_mass(targets)\nnewcollecteddf = RCC.merge_new_data()\nRCC.final_merge(newcollecteddf)\nRCC.create_metadata_to_push_recover()\nRCC.upload_recovered_to_kaggle()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:38:26.07564Z","iopub.execute_input":"2025-02-18T08:38:26.075983Z","iopub.status.idle":"2025-02-18T09:58:35.044609Z","shell.execute_reply.started":"2025-02-18T08:38:26.075951Z","shell.execute_reply":"2025-02-18T09:58:35.04289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"shutil.rmtree('/kaggle/working/daily_update')\nshutil.rmtree('/kaggle/working/daily_update_to_kaggle')\nif today.weekday() == 0 or force == True:\n    shutil.rmtree('/kaggle/working/per_minute')\n    shutil.rmtree('/kaggle/working/per_minute_to_kaggle')\n    shutil.rmtree('/kaggle/working/per_minute_recover')\n    shutil.rmtree('/kaggle/working/per_minute_to_kaggle_final')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T09:58:35.046567Z","iopub.execute_input":"2025-02-18T09:58:35.046959Z","iopub.status.idle":"2025-02-18T09:58:37.247037Z","shell.execute_reply.started":"2025-02-18T09:58:35.046918Z","shell.execute_reply":"2025-02-18T09:58:37.245968Z"}},"outputs":[],"execution_count":null}]}
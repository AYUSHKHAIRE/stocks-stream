{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, explode, arrays_zip\n",
    "from pyspark.sql.types import StructType, StructField, ArrayType, DoubleType, LongType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"StockPriceStream\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")  # Reduce logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for incoming JSON data\n",
    "schema = StructType([\n",
    "    StructField(\"unix_timestamp\", ArrayType(LongType()), True),\n",
    "    StructField(\"stockname\", ArrayType(StringType()), True),\n",
    "    StructField(\"open\", ArrayType(DoubleType()), True),\n",
    "    StructField(\"close\", ArrayType(DoubleType()), True),\n",
    "    StructField(\"high\", ArrayType(DoubleType()), True),\n",
    "    StructField(\"low\", ArrayType(DoubleType()), True),\n",
    "    StructField(\"volume\", ArrayType(DoubleType()), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stream from socket\n",
    "raw_stream = spark.readStream.format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 3456) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse JSON\n",
    "parsed_stream = raw_stream.select(from_json(col(\"value\"), schema).alias(\"data\")).select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip all arrays together\n",
    "zipped_stream = parsed_stream.withColumn(\"zipped\", arrays_zip(\n",
    "    col(\"unix_timestamp\"),\n",
    "    col(\"stockname\"),\n",
    "    col(\"open\"),\n",
    "    col(\"close\"),\n",
    "    col(\"high\"),\n",
    "    col(\"low\"),\n",
    "    col(\"volume\")\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode into a structured DataFrame\n",
    "flattened_stream = zipped_stream.select(\n",
    "    explode(col(\"zipped\")).alias(\"row\")\n",
    ").select(\n",
    "    col(\"row.unix_timestamp\").alias(\"unix_timestamp\"),\n",
    "    col(\"row.stockname\").alias(\"stockname\"),\n",
    "    col(\"row.open\").alias(\"open\"),\n",
    "    col(\"row.close\").alias(\"close\"),\n",
    "    col(\"row.high\").alias(\"high\"),\n",
    "    col(\"row.low\").alias(\"low\"),\n",
    "    col(\"row.volume\").alias(\"volume\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_query = flattened_stream.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"stock_current_data\") \\\n",
    "    .start()\n",
    "\n",
    "append_query = flattened_stream.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"path\", \"/home/ayushkhaire/code/dataennginneerinng/stocksly/stream-processing/output\") \\\n",
    "    .option(\"checkpointLocation\", \"/home/ayushkhaire/code/dataennginneerinng/stocksly/stream-processing/checkpoints\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/16 15:40:59 ERROR MicroBatchExecution: Query [id = f73b164b-dbf5-4d33-ad28-5d0189d752f3, runId = 29ccae40-577c-4e14-81cb-e0c79472b961] terminated with error\n",
      "java.lang.IndexOutOfBoundsException: at 0 deleting 68\n",
      "\tat scala.collection.mutable.ListBuffer.remove(ListBuffer.scala:273)\n",
      "\tat scala.collection.mutable.BufferLike.trimStart(BufferLike.scala:178)\n",
      "\tat scala.collection.mutable.BufferLike.trimStart$(BufferLike.scala:178)\n",
      "\tat scala.collection.mutable.AbstractBuffer.trimStart(Buffer.scala:50)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.TextSocketMicroBatchStream.commit(TextSocketMicroBatchStream.scala:162)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$commitSources$1(MicroBatchExecution.scala:568)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$commitSources$1$adapted(MicroBatchExecution.scala:565)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamProgress.foreach(StreamProgress.scala:27)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.commitSources(MicroBatchExecution.scala:565)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.cleanUpLastExecutedMicroBatch(MicroBatchExecution.scala:796)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$13(MicroBatchExecution.scala:544)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:535)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Wait for termination\n",
    "update_query.awaitTermination()\n",
    "append_query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
